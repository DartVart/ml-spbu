# ML course SPBU

## Task 1. Generation of Russian Names

    Here you can generate russian (and other) names. Based on [code](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb) from [@karpathy](https://github.com/karpathy).

### Training
To launch training go to `tasks/task1` directory and run:
```commandline
python train.py train.txt
```
where `train.txt` is a file that includes russian names to train a model.

Example of output:
```
Training started...
Training finished!
Train loss: 2.04
Trained model saved in the file "model.pth".
```

### Testing
To test a model go to `tasks/task1` directory and run:
```commandline
python test.py model.pth test.txt
```
where
* `model.pth` is a file that includes model weights
* `test.txt` is a file that includes russian names to test a model

Example of output:
```
Test loss: 2.37

Examples of names generated by the model:
горьенсина.
секж.
авгагуллуаниктий.
антей.
ботсинуфа.
едиля.
тара.
гелия.
альгобросиньмера.
глина.
```

## Task 2. Generation a Mayakovsky's poem

Here you can generate a Mayakovsky poem using AI.

### Models
Several models are available for generation:
* `old` — [baseline GPT model](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py), trained on a corpus of Mayakovsky's poems located in the `input.txt` file. This is the model with the best loss on the validation split (1000 iterations of training).
* `old_5000` — The baseline model trained on 5000 iterations. It's overfitted, but also shows interesting results.
* `new` — This is the basic model to which [Synthesizer](https://arxiv.org/pdf/2005.00743.pdf) has been added, combined with the basic self-attention algorithm. The best model, also taken after the 1000th iteration of training.
* `new_5000` — The new model trained on 5000 iterations. It's overfitted, but also shows interesting results.

### Generation
To generate a poem go to `tasks/task2` directory and run, for example:
```commandline
python generate.py new generated.txt 500
```
where
* `new` — the name of the model you want to use (it can also be `old`, `old_5000`, `new_5000`).
* `generated.txt` — the text in which the verse will be written.
* `500` — the length of the verse in characters.

Example of output:
```
Preparing a model...
The model is loaded!
Generating a text...
The text is generated!
========== NEW VERSE! ==========
проходится,
но надо
разлихо
и всех,
когда
законя гвоздернуя солнцев.
Со улицы храмати крепкий,
хлеб
шара!
Как что в пехоже коровав,
перекресть
слюновичи.
Трубки жи у ти -
скажды
они чулки,
потом,
как будто зубы спрошенния -
из горла без Эрнги!
У Степы крови
с дорога
на чагот часа,
а порого
где где не была
и черных цент,
и дешейте
солидут
в пензе тчеты, -
слушайте горды.
Где радом
жизнь
рагрово
вперед
мчивых прот.
Знакидывают
RsA Aвара
с пророкин порыщи.
Товарищ Жоресенс,
тропиром
тихо
про царя.
===============================
```

### What's in the files?

* `model.py` — the file where the model and its training are located.
* `input.txt` — the corpus of Mayakovsky's poems.
* `perplexities.png` — сomparison of perplexity for the `new` and `old` models.
* folders `old_model` and `new_model` contains data about the old and new models:
  * `[X]_iter` — the model trained on X iterations
  * `losses.csv` and `losses.png` — both files contain training and test losses
